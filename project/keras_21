#!/usr/bin/env python
"""
Example code for testing various techniques to assess impact on CIFAR10 accuracy.

References
----------
Kaiming He et al, Deep Residual Learning for Image Recognition, https://arxiv.org/abs/1512.03385
Keras team, https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py
Keras team, Image Preprocessing, https://keras.io/preprocessing/image/

__author__ = "Hide Inada"
__copyright__ = "Copyright 2018, Hide Inada"
__license__ = "The MIT License"
__email__ = "hideyuki@gmail.com"
"""

import os
import logging

import numpy as np
import sys

import keras
import keras.layers

from keras.models import Model
from keras.layers import Input
from keras.layers import Conv2D
from keras.layers import MaxPool2D
from keras.layers import Flatten
from keras.layers import Dense
from keras.layers import BatchNormalization
from keras.layers import GlobalAveragePooling2D
from keras.layers import ReLU
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator

log = logging.getLogger(__name__)
logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))  # Change the 2nd arg to INFO to suppress debug logging

EPOCHS = 100

def single_conv_block(tens, filters=None, kernel_size=None, strides=None):
    tens = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='SAME')(tens)
    tens = BatchNormalization()(tens)
    tens = ReLU()(tens)

    return tens

def single_conv_block_no_activation(tens, filters=None, kernel_size=None, strides=None):
    tens = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='SAME')(tens)
    tens = BatchNormalization()(tens)

    return tens

def single_resnet_block(tens, filters=None, kernel_size=None, downsample=False):
    original_tens = tens

    if downsample:
        original_tens = single_conv_block_no_activation(tens, filters=filters, kernel_size=1, strides=2)
        tens = single_conv_block(tens, filters=filters, kernel_size=kernel_size, strides=2)
    else:
        tens = single_conv_block(tens, filters=filters, kernel_size=kernel_size, strides=1)

    tens = single_conv_block_no_activation(tens, filters=filters, kernel_size=kernel_size, strides=1)

    tens = keras.layers.add([tens, original_tens])

    tens = ReLU()(tens)

    return tens


def n_resnet_blocks(num_blocks, tens, filters_prev=None, filters=None, kernel_size=None, downsample=False):

    if downsample:
        tens = single_resnet_block(tens, filters=filters, kernel_size=kernel_size, downsample=True)
        for i in range(num_blocks-1):
            tens = single_resnet_block(tens, filters=filters, kernel_size=kernel_size)
    else:
        for i in range(num_blocks):
            tens = single_resnet_block(tens, filters=filters, kernel_size=kernel_size)

    return tens


def train_and_predict():
    """Train the model and predict.

    This file downloads dataset via Keras util.
    """

    (x, y), (x_test, y_test) = keras.datasets.cifar10.load_data()

    # Change the value from 0<= x <= 255 in UINT8 to 0 <= x <= 1 in float
    x = x / 255.0
    x_test = x_test / 255.0

    y_oh = keras.utils.to_categorical(y, 10) * 1.0
    y_test_oh = keras.utils.to_categorical(y_test, 10)

    datagen = ImageDataGenerator(
        featurewise_center=False,
        featurewise_std_normalization=False,
        rotation_range=0,  # no rotation
        width_shift_range=4.0, # -4 to +4
        height_shift_range=4.0, # -4 to +4
        horizontal_flip=True)

    input_tens = Input(shape=(32, 32, 3))

    tens = single_conv_block(input_tens, filters=64, kernel_size=3, strides=1)

    tens = n_resnet_blocks(2, tens, filters=64, kernel_size=3)
    tens = n_resnet_blocks(2, tens, filters=128, kernel_size=3, downsample=True)
    tens = n_resnet_blocks(2, tens, filters=256, kernel_size=3, downsample=True)

    tens = GlobalAveragePooling2D()(tens)

    last_tens = Dense(10, activation='softmax')(tens)

    model = Model(input_tens, last_tens)
    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
    model.compile(adam, loss=keras.losses.categorical_crossentropy, metrics=[keras.metrics.categorical_accuracy])

    datagen.fit(x)
    model.fit_generator(datagen.flow(x, y_oh, batch_size=128),
                        steps_per_epoch=len(x) / 128, epochs=EPOCHS)

    #model.fit(x, y_oh, epochs=EPOCHS, batch_size=128)

    # predict
    y_hat_oh = model.predict(x_test)
    total_size = y_hat_oh.shape[0]
    y_hat_int = np.argmax(y_hat_oh, axis=1)  # to int from one-hot vector
    y_hat_int = y_hat_int.reshape((y_hat_int.shape[0], 1))
    matched_indices = (y_hat_int == y_test)
    matched_count = y_test[matched_indices].shape[0]
    print("Matched: %d out of Total: %d (%f percent)" % (matched_count, total_size, matched_count * 100 / total_size))


def main():
    """Defines an application's main functionality"""
    train_and_predict()


if __name__ == "__main__":
    main()
