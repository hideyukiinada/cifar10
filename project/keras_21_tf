#!/usr/bin/env python
"""
Example code for testing various techniques to assess impact on CIFAR10 accuracy.

__author__ = "Hide Inada"
__copyright__ = "Copyright 2019, Hide Inada"
__license__ = "The MIT License"
__email__ = "hideyuki@gmail.com"
"""

import os
import logging
from pathlib import Path
import numpy as np
import tensorflow as tf
import keras

log = logging.getLogger(__name__)
logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))  # Change the 2nd arg to INFO to suppress debug logging

IGNORE_WEIGHT = True
BASE_DIR = "/tmp/cifar10/21tf"
WEIGHTS_DIR_PATH = Path(BASE_DIR) / Path("cp")
TF_LOG_DIR_PATH = Path(BASE_DIR) / Path("log")

EPOCHS = 100
BATCH_SIZE = 64


def single_conv_block(tens, block_name, input_filters, output_filters, kernel_size=3, padding="SAME", strides=1,
                      is_training=True):
    scope_name = block_name + "_cv"
    with tf.variable_scope(scope_name) as scope:
        weights = tf.get_variable("weights", [kernel_size, kernel_size, input_filters, output_filters],
                                  dtype=tf.float32,
                                  initializer=tf.contrib.layers.xavier_initializer(seed=0))
        bias = tf.get_variable("bias", [output_filters], dtype=tf.float32)

        z = tf.add(tf.nn.conv2d(tens, weights, strides=[1, strides, strides, 1], padding=padding), bias)

        bn = tf.layers.batch_normalization(z, training=is_training)

        activation = tf.nn.relu(bn, name=scope.name)

        return activation


def single_conv_block_no_activation(tens, block_name, input_filters, output_filters, kernel_size=3, padding="SAME",
                                    strides=1,
                                    is_training=True):
    scope_name = block_name + "_cv"
    with tf.variable_scope(scope_name) as scope:
        weights = tf.get_variable("weights", [kernel_size, kernel_size, input_filters, output_filters],
                                  dtype=tf.float32,
                                  initializer=tf.contrib.layers.xavier_initializer(seed=0))
        bias = tf.get_variable("bias", [output_filters], dtype=tf.float32)

        z = tf.add(tf.nn.conv2d(tens, weights, strides=[1, strides, strides, 1], padding=padding), bias)

        bn = tf.layers.batch_normalization(z, training=is_training)

        return bn


def single_resnet_block(tens, block_name, input_filters=None, output_filters=None, kernel_size=None, downsample=False,
                        is_training=True):
    original_tens = tens

    conv_block_name = block_name + "_1"
    if downsample:
        # if downsample, use the the number of filters for this block (input_filters)
        original_tens = single_conv_block_no_activation(tens, block_name + "_1_skip", input_filters=input_filters,
                                                        output_filters=output_filters, kernel_size=3, strides=2,
                                                        is_training=is_training)

        tens = single_conv_block(tens, conv_block_name, input_filters=input_filters, output_filters=output_filters,
                                 kernel_size=kernel_size, strides=2, is_training=is_training)
    else:
        tens = single_conv_block(tens, conv_block_name, input_filters=output_filters, output_filters=output_filters,
                                 kernel_size=kernel_size, strides=1, is_training=is_training)

    conv_block_name = block_name + "_2"
    tens = single_conv_block_no_activation(tens, conv_block_name, input_filters=output_filters,
                                           output_filters=output_filters, kernel_size=kernel_size, strides=1,
                                           is_training=is_training)

    tens = tf.add(tens, original_tens)

    tens = tf.nn.relu(tens, name=block_name + "_relu")

    return tens


def n_resnet_blocks(tens, block_name, num_blocks, input_filters, output_filters, kernel_size=None, downsample=False,
                    is_training=True):
    if downsample:
        tens = single_resnet_block(tens, block_name + "_1", input_filters, output_filters, kernel_size=kernel_size,
                                   downsample=True, is_training=is_training)
        for i in range(num_blocks - 1):
            tens = single_resnet_block(tens, block_name + "_" + str(i + 2), output_filters, output_filters,
                                       kernel_size=kernel_size,
                                       is_training=is_training)
    else:
        for i in range(num_blocks):
            tens = single_resnet_block(tens, block_name + "_" + str(i + 1), output_filters, output_filters,
                                       kernel_size=kernel_size,
                                       is_training=is_training)

    return tens


def build_network(is_training=True):
    tf.reset_default_graph()

    x_placeholder = tf.placeholder(tf.float32, shape=(None, 32, 32, 3))
    y_placeholder = tf.placeholder(tf.int32, shape=(None))

    input_tens = x_placeholder

    tens = tf.image.resize_images(input_tens, (36, 36), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    tens = tf.image.random_crop(tens, [tf.shape(tens)[0], 32, 32, 3])
    tens = tf.image.random_flip_left_right(tens)

    tens = single_conv_block(tens, "Initial_conv_block", input_filters=3, output_filters=64, kernel_size=3,
                             strides=1, is_training=is_training)

    tens = n_resnet_blocks(tens, "64_block", 2, input_filters=64, output_filters=64, kernel_size=3,
                           is_training=is_training)
    tens = n_resnet_blocks(tens, "128_block", 2, input_filters=64, output_filters=128, kernel_size=3, downsample=True,
                           is_training=is_training)
    tens = n_resnet_blocks(tens, "256_block", 2, input_filters=128, output_filters=256, kernel_size=3, downsample=True,
                           is_training=is_training)

    tens = tf.reduce_mean(tens, axis=[1, 2])  # global average pooling

    tens = tf.layers.dense(
        tens,
        10,
        activation=None,
        use_bias=True,
        kernel_initializer=tf.contrib.layers.xavier_initializer(seed=0))

    y_hat_softmax = tf.nn.softmax(tens)

    # Set up optimizer & cost
    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)
    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tens, labels=y_placeholder))

    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        objective = optimizer.minimize(cost)

    with tf.variable_scope("gs") as scope:
        global_step = tf.train.get_or_create_global_step()
        inc_gs = tf.assign(global_step, global_step + 1)

    tf.summary.scalar('Cost', cost)
    merged = tf.summary.merge_all()

    init_op = tf.global_variables_initializer()  # Set up operator to assign all init values to variables
    saver = tf.train.Saver()
    return init_op, cost, objective, x_placeholder, y_placeholder, y_hat_softmax, saver, merged, inc_gs, global_step


def load_data():
    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

    x_train = x_train / 255.0
    x_test = x_test / 255.0

    y_train = y_train.reshape(y_train.shape[0])

    y_train_oh = keras.utils.to_categorical(y_train, 10) * 1.0
    y_test_oh = keras.utils.to_categorical(y_test, 10)

    return x_train, y_train, x_test, y_test


def train(x_train, y_train, x_test, y_test):
    """Train the model and predict.

    This file downloads dataset via Keras util.
    """

    # Create Tensorboard log directory
    tf_log_dir_path = TF_LOG_DIR_PATH
    if tf_log_dir_path.exists() is False:
        tf_log_dir_path.mkdir(parents=True, exist_ok=True)

    init_op, cost, objective, x_placeholder, y_placeholder, y_hat_softmax, saver, merged, inc_gs, global_step = build_network()

    with tf.Session() as s:

        tensorboard_writer = tf.summary.FileWriter(tf_log_dir_path, s.graph)

        if WEIGHTS_DIR_PATH.exists() and IGNORE_WEIGHT is False:
            saver.restore(s, str(WEIGHTS_DIR_PATH / Path("model.ckpt")))
            log.info("Loaded weight from: %s" % str(WEIGHTS_DIR_PATH / Path("model.ckpt")))
        else:
            s.run(init_op)

        dataset_size = x_train.shape[0]

        for i in range(EPOCHS):
            next_k = 0
            loop_count = int(
                dataset_size / BATCH_SIZE)  # for m = 5, batch_size = 2, this results in [0, 1]
            current_batch_size = 0

            for j in range(loop_count):
                current_batch_size = BATCH_SIZE
                k = j * current_batch_size
                next_k = k + current_batch_size

                merged_result, _, gs, o, c = s.run([merged, inc_gs, global_step, objective, cost],
                                                   feed_dict={x_placeholder: x_train[k:next_k],
                                                              y_placeholder: y_train[k:next_k]})

                tensorboard_writer.add_summary(merged_result, gs)

                log.info("[%d] Epoch: %d.  Batch: %d Cost:%f, Batch size: %d" % (gs, i, j, c, current_batch_size))

            # remainder
            last_batch_size = x_train.shape[0] - next_k
            if last_batch_size > 0:
                k = next_k

                merged_result, _, gs, o, c = s.run([merged, inc_gs, global_step, objective, cost],
                                                   feed_dict={x_placeholder: x_train[k:k + last_batch_size],
                                                              y_placeholder: y_train[k:k + last_batch_size]})

                tensorboard_writer.add_summary(merged_result, gs)
                log.info("[%d] Epoch: %d.  Batch: %d Cost:%f, Batch size: %d" % (gs, i, j, c, last_batch_size))

        log.info("Training completed.")

        if WEIGHTS_DIR_PATH.exists() is False:
            WEIGHTS_DIR_PATH.mkdir(parents=True, exist_ok=True)

        weight_path = saver.save(s, str(WEIGHTS_DIR_PATH / Path("model.ckpt")))
        log.info("Saved model in: %s" % weight_path)


def predict(x_test, y_test):
    init_op, cost, objective, x_placeholder, y_placeholder, y_hat_softmax, saver, merged, inc_gs, global_step = build_network(
        is_training=False)

    with tf.Session() as s:
        try:
            saver.restore(s, str(WEIGHTS_DIR_PATH / Path("model.ckpt")))
            log.info("Loaded weight from: %s" % str(WEIGHTS_DIR_PATH / Path("model.ckpt")))
        except:
            log.fatal("Exception while restoring weight.")
            raise ValueError("Exception while restoring weight.")

        dataset_size = x_test.shape[0]

        y_hat_list = list()

        next_k = 0
        loop_count = int(
            dataset_size / BATCH_SIZE)  # for m = 5, batch_size = 2, this results in [0, 1]
        current_batch_size = 0

        for j in range(loop_count):
            current_batch_size = BATCH_SIZE
            k = j * current_batch_size
            next_k = k + current_batch_size

            y_hat_batch = s.run(y_hat_softmax, feed_dict={x_placeholder: x_test[k:next_k]})
            y_hat_list.append(y_hat_batch)

        # remainder
        last_batch_size = x_test.shape[0] - next_k
        if last_batch_size > 0:
            k = next_k

            y_hat_batch = s.run(y_hat_softmax, feed_dict={
                x_placeholder: x_test[k:k + last_batch_size]})

            y_hat_list.append(y_hat_batch)

    y_hat = np.concatenate(y_hat_list)

    if y_hat.shape[0] != y_test.shape[0]:
        log.fatal("y hat shape: " + str(y_hat.shape))
        log.fatal("y test shape: " + str(y_test.shape))
        raise ValueError("Shape mismatch between y hat and y")

    total_size = y_hat.shape[0]
    y_hat_int = np.argmax(y_hat, axis=1)  # to int from one-hot vector
    y_hat_int = y_hat_int.reshape((y_hat_int.shape[0], 1))

    matched_indices = (y_hat_int == y_test)
    matched_count = y_test[matched_indices].shape[0]
    log.info(
        "Matched: %d out of Total: %d (%f percent)" % (matched_count, total_size, matched_count * 100 / total_size))


def main():
    """Defines an application's main functionality"""

    x_train, y_train, x_test, y_test = load_data()

    train(x_train, y_train, x_test, y_test)
    predict(x_test, y_test)


if __name__ == "__main__":
    main()
